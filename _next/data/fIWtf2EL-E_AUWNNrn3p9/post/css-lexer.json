{"pageProps":{"title":"从 CSS 字符串到 AST（一）—— 词法分析器（Lexer）的实现 - 宝硕博客","post":{"title":"从 CSS 字符串到 AST（一）—— 词法分析器（Lexer）的实现","excerpt":"最近在实习的时候，遇到了一些需求，需要自己去实现 CSS 的解析、（伪）渲染流程。以之为契机，我学习了一下编译相关的知识，其中的第一环就是 Lexer。","content":[["$","p",{},["最近在实习的时候，遇到了一些需求，需要自己去实现 CSS 的解析、（伪）渲染流程。以之为契机，我学习了一下编译相关的知识，其中的第一环就是 Lexer。"]],["$","span",{"id":"more"},[]],["$","p",{},["本文中的代码均使用 Go 实现，成果已经作为 Go 库 ",["$","a",{"href":"https://pkg.go.dev/go.baoshuo.dev/csslexer","rel":"external nofollow noreferrer"},[["$","code",{},["go.baoshuo.dev/csslexer"]]]]," 发布。"]],["$","ul",{},[["$","li",{},["GitHub 仓库：",["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer","rel":"external nofollow noreferrer"},["renbaoshuo/go-css-lexer"]]]],["$","li",{},["实现标准：",["$","a",{"href":"https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/","rel":"external nofollow noreferrer"},["CSS Syntax Module Level 3 (W3C Candidate Recommendation Draft; 24 December 2021)"]]]]]],["$","p",{},[["$","em",{},["建议在阅读本文前对 CSS 标准内容有一定理解。"]]]],["$","h2",{"id":"词法分析"},["词法分析"]],["$","blockquote",{},[["$","p",{},["词法分析（lexical analysis）是计算机科学中将字符序列转换为记号（token，也有译为标记或词元）序列的过程。进行词法分析的程序或者函数叫作词法分析器（lexical analyzer，简称 lexer），也叫扫描器（scanner）。词法分析器一般以函数的形式存在，供语法分析器调用。"]],["$","p",{},[["$","cite",{},["——维基百科"]]]]]],["$","p",{},["词法分析是编译中的第一个步骤。它读入组成源码的字符流，并将他们组织成一个个的词素（lexeme）。有了词素以后，识别并标注它的类型，就可以生成一个 ",["$","code",{},["<token-name, attribute-value>"]]," 形式的词法单元（token）。这个单元会被传送给下一个步骤 —— 语法分析 —— 进行后续的处理。"]],["$","p",{},["在进行词法分析之前，首先要设定好到底有多少种 token 类型，然后再确定每个 token 类型的判断条件和解析方式。"]],["$","h2",{"id":"token-的分类"},["Token 的分类"]],["$","p",{},["由 CSS Syntax Module Level 3 中的 ",["$","a",{"href":"https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#tokenization","rel":"external nofollow noreferrer"},["4. Tokenization"]]," 一节可以得到 CSS 的 token 有以下几种类型："]],["$","pre",{},[["$","code",{"class":"hljs text"},["<ident-token>\n<function-token>\n<at-keyword-token>\n<hash-token>\n<string-token>\n<bad-string-token>\n<url-token>\n<bad-url-token>\n<delim-token>\n<number-token>\n<percentage-token>\n<dimension-token>\n<whitespace-token>\n<CDO-token>\n<CDC-token>\n<colon-token>\n<semicolon-token>\n<comma-token>\n<[-token>\n<]-token>\n<(-token>\n<)-token>\n<{-token>\n<}-token>"]]]],["$","p",{},["为了解析方便，我们又在标准的 token 类型外拓展了几个 token 类型，得到了下面的 token 表："]],["$","pre",{},[["$","code",{"class":"hljs text"},["<ident-token>         IdentToken\n<function-token>      FunctionToken            foo()\n<at-keyword-token>    AtKeywordToken           @foo\n<hash-token>          HashToken                #foo\n<string-token>        StringToken\n<bad-string-token>    BadStringToken\n<url-token>           UrlToken                 url()\n<bad-url-token>       BadUrlToken\n<delim-token>         DelimiterToken\n<number-token>        NumberToken              3\n<percentage-token>    PercentageToken          3%\n<dimension-token>     DimensionToken           3em\n<whitespace-token>    WhitespaceToken\n<CDO-token>           CDOToken                 <!--\n<CDC-token>           CDCToken                 -->\n<colon-token>         ColonToken               :\n<semicolon-token>     SemicolonToken           ;\n<comma-token>         CommaToken               ,\n<(-token>             LeftParenthesisToken     (\n<)-token>             RightParenthesisToken    )\n<[-token>             LeftBracketToken         [\n<]-token>             RightBracketToken        ]\n<{-token>             LeftBraceToken           {\n<}-token>             RightBraceToken          }\n<EOF-token>           EOFToken\n\nCommentToken          /* ... */\nIncludeMatchToken     ~=\nDashMatchToken        |=\nPrefixMatchToken      ^=\nSuffixMatchToken      $=\nSubstringMatchToken   *=\nColumnToken           ||\nUnicodeRangeToken"]]]],["$","p",{},["于是乎，我们就有了词法分析的期望目标产物 —— 由这 33 种类型的 token 组成的 token 流。"]],["$","h2",{"id":"输入流"},["输入流"]],["$","blockquote",{},[["$","p",{},["工欲善其事，必先利其器。"]]]],["$","p",{},["在实现真正的词法分析流程以前，我们需要编写一套输入流来辅助我们完成读入的操作。"]],["$","p",{},["首先，我们给出输入流的定义："]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// Input represents a stream of runes read from a source."]],"\n",["$","span",{"class":"hljs-keyword"},["type"]]," Input ",["$","span",{"class":"hljs-keyword"},["struct"]]," {\n    runes []",["$","span",{"class":"hljs-type"},["rune"]]," ",["$","span",{"class":"hljs-comment"},["// The runes in the input stream."]],"\n    pos   ",["$","span",{"class":"hljs-type"},["int"]],"    ",["$","span",{"class":"hljs-comment"},["// The current position in the input stream."]],"\n    start ",["$","span",{"class":"hljs-type"},["int"]],"    ",["$","span",{"class":"hljs-comment"},["// The start position of the current token being read."]],"\n    err   ",["$","span",{"class":"hljs-type"},["error"]],"  ",["$","span",{"class":"hljs-comment"},["// Any error encountered while reading the input."]],"\n}"]]]],["$","p",{},["这个结构封装了对一个 rune 切片的访问，并维护了当前扫描的位置（pos）和当前正在扫描的 token 的起始位置（start）。"]],["$","p",{},["需要注意的是，我们使用 ",["$","code",{},["rune"]]," 而不是 ",["$","code",{},["byte"]]," 来存储内容，这样做的原因是为了便于处理代码中包含的 Emoji 等 Unicode 字符。"]],["$","p",{},["为了使用方便，这个输入流可以从 ",["$","code",{},["string"]],"、",["$","code",{},["[]rune"]],"、",["$","code",{},["[]byte"]]," 和 ",["$","code",{},["io.Reader"]]," 初始化。实现细节可以查看仓库中的 ",["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/master/input.go","rel":"external nofollow noreferrer"},[["$","code",{},["input.go"]]]],"，各个函数签名如下："]],["$","ul",{},[["$","li",{},[["$","code",{},["NewInput(input string) *Input"]]]],["$","li",{},[["$","code",{},["NewInputRunes(runes []rune) *Input"]]]],["$","li",{},[["$","code",{},["NewInputBytes(input []byte) *Input"]]]],["$","li",{},[["$","code",{},["NewInputReader(r io.Reader) *Input"]]]]]],["$","p",{},["接下来，我们需要设计一系列合理的方法，使得这个输入流的使用能够在满足我们的实际需求的同时，还保持简洁的风格。"]],["$","p",{},["在 4.2 节的一系列定义中，通过观察不难发现，在解析过程中会不断地出现 consume 和 reconsume 的操作，也就是说，在输入流的末尾会不断地进行 pop_back 和 push_back 的操作。那么我们可以将这些操作转化为「预读」和「后移指针」的操作，以此来减少频繁在流末尾进行的弹出和插入操作。"]],["$","p",{},["于是，我们就有了以下两个方法："]],["$","ul",{},[["$","li",{},[["$","p",{},[["$","code",{},["func (z *Input) Peek(n int) rune"]]]],["$","p",{},["预读输入流中 ",["$","code",{},["pos+n"]]," 位置的字符。"]]]],["$","li",{},[["$","p",{},[["$","code",{},["func (z *Input) Move(n int)"]]]],["$","p",{},["将当前输入流的指针后移 ",["$","code",{},["n"]]," 位。"]]]]]],["$","p",{},["经过阅读规范以后，不难发现一个 token 可以由几个不同类别的字符序列组成，比如 ",["$","code",{},["16px"]]," 就是一个 ",["$","code",{},["16"]]," (number sequence) 和一个 ",["$","code",{},["px"]]," (ident sequence) 共同组成的 dimension-token。所以我们在解析一个 token 的时候可能会调用多个解析函数，那么就需要在 token 级别做一个固定的输出模式。"]],["$","p",{},["于是，我们定义 ",["$","code",{},["func (z *Input) Shift() []rune"]]," 来弹出当前 token，并更新 ",["$","code",{},["Input"]]," 实例中的 ",["$","code",{},["start"]]," 值，以开始下一 token 的解析。"]],["$","p",{},["不过后续在解析 url-token 的时候遇到了需要读取当前已经 consume 的内容的情况，于是将 ",["$","code",{},["Shift"]]," 方法拆分成了 ",["$","code",{},["Current"]]," 和 ",["$","code",{},["Shift"]]," 两个不同的方法，以便使用。"]],["$","p",{},["除此以外，在解析的时候还有需要在满足某一特定条件下一直 consume 的能力需求，因此又设计了较为通用的 ",["$","code",{},["func (z *Input) MoveWhilePredicate(pred func(rune) bool)"]]," 方法，来实现这一能力。"]],["$","p",{},["加上错误处理逻辑以后，整个 ",["$","code",{},["Input"]]," 的方法如下："]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," PeekErr(pos ",["$","span",{"class":"hljs-type"},["int"]],") ",["$","span",{"class":"hljs-type"},["error"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," Err() ",["$","span",{"class":"hljs-type"},["error"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," Peek(n ",["$","span",{"class":"hljs-type"},["int"]],") ",["$","span",{"class":"hljs-type"},["rune"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," Move(n ",["$","span",{"class":"hljs-type"},["int"]],")\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," Current() []",["$","span",{"class":"hljs-type"},["rune"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," Shift() []",["$","span",{"class":"hljs-type"},["rune"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(z *Input)"]]]]," MoveWhilePredicate(pred ",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]],["$","span",{"class":"hljs-params"},["(",["$","span",{"class":"hljs-type"},["rune"]],")"]]]]," ",["$","span",{"class":"hljs-type"},["bool"]],")"]]]],["$","p",{},["接下来，我们就可以正式开始 lexer 的编写了。"]],["$","h2",{"id":"词法分析器"},["词法分析器"]],["$","p",{},["其实 Lexer 的方法框架设计就相对简单了，下面直接给出定义："]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-keyword"},["type"]]," Lexer ",["$","span",{"class":"hljs-keyword"},["struct"]]," {\n    r *Input ",["$","span",{"class":"hljs-comment"},["// The input stream of runes to be lexed."]],"\n}\n\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," Err() ",["$","span",{"class":"hljs-type"},["error"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," Next() (TokenType, []",["$","span",{"class":"hljs-type"},["rune"]],")"]]]],["$","p",{},["在 ",["$","code",{},["Next"]]," 方法中有一个巨大的 switch-case 语句，这里面包含了 ",["$","a",{"href":"https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-token","rel":"external nofollow noreferrer"},["4.3.1. Consume a token"]]," 中所描述的所有在 token 开始时的情形。我们将会根据一个 token 开始的几个字符（小于等于 3 个）来确定这个 token 的后续部分应该如何解析。"]],["$","h3",{"id":"token-开始处的分类讨论"},["Token 开始处的分类讨论"]],["$","p",{},["开始解析 token 的时候一定是在文件流的开头或者上一个 token 刚刚解析完毕的时候，那么此时我们只需要根据对应规则判断 token 类型即可。"]],["$","p",{},["首先预读 1 个字符，记为 ",["$","code",{},["next"]],"，然后对这个字符进行分类讨论。"]],["$","ul",{},[["$","li",{},[["$","code",{},["EOF"]],"：直接返回 EOF-token。"]],["$","li",{},[["$","code",{},["\\t"]],", ",["$","code",{},["\\n"]],", ",["$","code",{},["\\r"]],", ",["$","code",{},["\\f"]],", ",["$","code",{},[]],"：根据标准需要将此字符及后续的所有 whitespace 组合成一个 whitespace-token。"]],["$","li",{},[["$","code",{},["/"]],"：如果是 ",["$","code",{},["/*"]]," 则一直读取到 ",["$","code",{},["*/"]]," 或者 ",["$","code",{},["EOF"]]," 作为 comment-token。"]],["$","li",{},[["$","code",{},["'"]]," (单引号), ",["$","code",{},["\""]],"(双引号)：遇到这两种引号，会调用字符串解析函数 ",["$","code",{},["consumeStringToken()"]],"。该函数会持续读取字符，直到遇到与之匹配的结束引号。在此过程中，它会处理转义字符（如 ",["$","code",{},["\\\""]],"）。如果在中途遇到换行符或文件末尾，则会生成一个 bad-string-token，否则生成一个 string-token。"]],["$","li",{},[["$","code",{},["0"]]," ~ ",["$","code",{},["9"]]," 的数字字符：如果以数字开头，确定无疑是数字类型，调用数字解析函数 ",["$","code",{},["consumeNumericToken()"]],"。"]],["$","li",{},[["$","code",{},["("]],", ",["$","code",{},[")"]],", ",["$","code",{},["["]],", ",["$","code",{},["]"]],", ",["$","code",{},["{"]],", ",["$","code",{},["}"]],"：生成对应的括号字符。function-token 或者 url-token 的情况会在处理 ident-like 的时候另行考虑。"]],["$","li",{},[["$","code",{},["+"]],", ",["$","code",{},["."]],"：这两个字符，再加上 ",["$","code",{},["-"]],"，都比较特殊。不过 ",["$","code",{},["-"]]," 需要包含一些额外的判断，因此归属于另外一条规则处理。",["$","ul",{},[["$","li",{},["解析器会向后预读，通过 ",["$","code",{},["nextCharsAreNumber()"]]," 判断后续字符是否能构成一个合法的数字（例如 ",["$","code",{},["+1.5"]],", ",["$","code",{},[".5"]],"）。"]],["$","li",{},["如果可以，则调用 ",["$","code",{},["consumeNumericToken()"]]," 将其完整解析为一个 numeric-token。"]],["$","li",{},["如果不构成数字，则 ",["$","code",{},["+"]]," 和 ",["$","code",{},["."]]," 会被当作 delimiter-token。"]]]]]],["$","li",{},[["$","code",{},["-"]],"：除了像 ",["$","code",{},["+"]]," 一样判断是否有可能进入数字的处理逻辑以外，还需要考虑作为 ",["$","code",{},["-->"]]," (CDC-token) 和 ident-like 的情况。如果都不是才会被当做 delimiter-token。"]],["$","li",{},[["$","code",{},["<"]],"：如果能构成 ",["$","code",{},["<!--"]],"，解析为一个 CDO-token，否则解析为 delimiter-token。"]],["$","li",{},[["$","code",{},["*"]],", ",["$","code",{},["^"]],", ",["$","code",{},["$"]],", ",["$","code",{},["|"]],", ",["$","code",{},["~"]],": 这些是属性选择器中的匹配符。",["$","ul",{},[["$","li",{},["如果它们后面紧跟 ",["$","code",{},["="]],"，则会组合成一个专有 token：",["$","ul",{},[["$","li",{},[["$","code",{},["*="]]," → substring-match-token"]],["$","li",{},[["$","code",{},["^="]]," → prefix-match-token"]],["$","li",{},[["$","code",{},["$="]]," → suffix-match-token"]],["$","li",{},[["$","code",{},["~="]]," → include-match-token"]],["$","li",{},[["$","code",{},["|="]]," → dash-match-token"]]]]]],["$","li",{},["特别地，对于 ",["$","code",{},["|"]],"，如果能够组成 ",["$","code",{},["||"]],"，则会成为 column-token。"]],["$","li",{},["如果没有，则单独作为 delimiter-token。"]]]]]],["$","li",{},[["$","code",{},["@"]],"：如果后续的字符能够组成一个 identifier，那么解析为 at-keyword-token，否则解析为 delimiter-token。"]],["$","li",{},[["$","code",{},[","]]," (逗号)：直接生成 comma-token。"]],["$","li",{},[["$","code",{},[":"]]," (冒号)：直接生成 colon-token。"]],["$","li",{},[["$","code",{},[";"]]," (分号)：直接生成 semicolon-token。"]],["$","li",{},[["$","code",{},["u"]]," 或 ",["$","code",{},["U"]],"：这是一个特殊前缀。如果其后是 + 紧跟着十六进制数字或 ? (例如 U+26 或 u+A?)，则调用 ",["$","code",{},["consumeUnicodeRangeToken()"]]," 解析为一个 urange-token。否则，按标识符处理。",["$","ul",{},[["$","li",{},["这里有一个坑点，需要在编写 parser 的时候注意，比如 ",["$","code",{},["u+a"]]," 既是一个合法的 unicode-range，也是一个合法的 selector，需要根据上下文来判定。"]]]]]],["$","li",{},["1 <= c <= 31, ",["$","code",{},["!"]],", ",["$","code",{},["%"]],", ",["$","code",{},["&"]],", ",["$","code",{},["="]],", ",["$","code",{},[">"]],", ",["$","code",{},["?"]],", ",["$","code",{},["`"]],", 127：解析为 delimiter-token。"]],["$","li",{},["其余字符：尝试解析为 ident-like。"]]]],["$","p",{},["整个流程在 ",["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/lexer.go#L24-L198","rel":"external nofollow noreferrer"},[["$","code",{},["lexer.go"]]," 的 24-198 行"]],"，由于篇幅原因此处就不贴代码了。"]],["$","h3",{"id":"token-解析"},["Token 解析"]],["$","p",{},["为了方便，我们为几种逻辑复杂 / 需要重用的 token 解析逻辑进行了封装，产生了如下函数："]],["$","ul",{},[["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume_token.go#L3-L16","rel":"external nofollow noreferrer"},[["$","code",{},["consumeNumericToken()"]]]]]],["$","ul",{},[["$","li",{},["先 consume 一个数字；"]],["$","li",{},["如果后续跟一个合法的 name，则 consume 这个 name 作为它的单位，组合为 dimension-token；"]],["$","li",{},["如果后续跟一个 ",["$","code",{},["%"]],"，consume 掉这个 ",["$","code",{},["%"]],"，产生一个 percentage-token；"]],["$","li",{},["否则产生一个 number-token。"]]]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-numeric-token"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeNumericToken() (TokenType, []",["$","span",{"class":"hljs-type"},["rune"]],") {\n    l.consumeNumber()\n\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," l.nextCharsAreIdentifier() {\n        l.consumeName()\n        ",["$","span",{"class":"hljs-keyword"},["return"]]," DimensionToken, l.r.Shift()\n    } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") == ",["$","span",{"class":"hljs-string"},["'%'"]]," {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume '%'"]],"\n        ",["$","span",{"class":"hljs-keyword"},["return"]]," PercentageToken, l.r.Shift()\n    }\n\n    ",["$","span",{"class":"hljs-keyword"},["return"]]," NumberToken, l.r.Shift()\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume_token.go#L18-L43","rel":"external nofollow noreferrer"},[["$","code",{},["consumeUnicodeRangeToken()"]]]]]],["$","ul",{},[["$","li",{},["有以下几种情况：",["$","ul",{},[["$","li",{},[["$","code",{},["U+0000FF"]],"，",["$","code",{},["+"]]," 后面可以跟 1 ~ 6 个 16 进制数字；"]],["$","li",{},[["$","code",{},["U+0000??"]],"，",["$","code",{},["+"]]," 后面先跟 16 进制数字再跟 ",["$","code",{},["?"]],"（通配符），总数不超过 6 个；"]],["$","li",{},[["$","code",{},["U+0001-0002"]],"，",["$","code",{},["-"]]," 两侧可以有 1 ~ 6 个 16 进制数字。"]]]]]],["$","li",{},["这些情况需要各自分类讨论，最后产生一个 urange-token。"]]]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#urange"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeUnicodeRangeToken() (TokenType, []",["$","span",{"class":"hljs-type"},["rune"]],") {\n    ",["$","span",{"class":"hljs-comment"},["// range start"]],"\n    start_length_remaining := ",["$","span",{"class":"hljs-number"},["6"]],"\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],"); start_length_remaining > ",["$","span",{"class":"hljs-number"},["0"]]," && next != EOF && isASCIIHexDigit(next); next = l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the hex digit"]],"\n        start_length_remaining--\n    }\n\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," start_length_remaining > ",["$","span",{"class":"hljs-number"},["0"]]," && l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") == ",["$","span",{"class":"hljs-string"},["'?'"]]," { ",["$","span",{"class":"hljs-comment"},["// wildcard range"]],"\n        ",["$","span",{"class":"hljs-keyword"},["for"]]," start_length_remaining > ",["$","span",{"class":"hljs-number"},["0"]]," && l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") == ",["$","span",{"class":"hljs-string"},["'?'"]]," {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the '?'"]],"\n            start_length_remaining--\n        }\n    } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") == ",["$","span",{"class":"hljs-string"},["'-'"]]," && isASCIIHexDigit(l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")) { ",["$","span",{"class":"hljs-comment"},["// range end"]],"\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the '-'"]],"\n\n        end_length_remaining := ",["$","span",{"class":"hljs-number"},["6"]],"\n        ",["$","span",{"class":"hljs-keyword"},["for"]]," next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],"); end_length_remaining > ",["$","span",{"class":"hljs-number"},["0"]]," && next != EOF && isASCIIHexDigit(next); next = l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the hex digit"]],"\n            end_length_remaining--\n        }\n    }\n\n    ",["$","span",{"class":"hljs-keyword"},["return"]]," UnicodeRangeToken, l.r.Shift()\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume_token.go#L47-L68","rel":"external nofollow noreferrer"},[["$","code",{},["consumeIdentLikeToken()"]]]]]],["$","ul",{},[["$","li",{},["先 consume 一个合法的 name；"]],["$","li",{},["然后判断是否为一个函数的开始，如果是，再判断是否是 url-token，转入特定的解析流程。",["$","ul",{},[["$","li",{},["需要额外注意的是，如果 url 函数的参数是使用单 / 双引号包裹的字符串，那么按照普通函数参数解析即可。"]]]]]]]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-ident-like-token"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeIdentLikeToken() (TokenType, []",["$","span",{"class":"hljs-type"},["rune"]],") {\n    l.consumeName()\n\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") == ",["$","span",{"class":"hljs-string"},["'('"]]," {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the opening parenthesis"]],"\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," equalIgnoringASCIICase(l.r.Current(), urlRunes) {\n            ",["$","span",{"class":"hljs-comment"},["// The spec is slightly different so as to avoid dropping whitespace"]],"\n            ",["$","span",{"class":"hljs-comment"},["// tokens, but they wouldn't be used and this is easier."]],"\n            l.consumeWhitespace()\n\n            next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," next != ",["$","span",{"class":"hljs-string"},["'\"'"]]," && next != ",["$","span",{"class":"hljs-string"},["'\\''"]]," {\n                ",["$","span",{"class":"hljs-keyword"},["return"]]," l.consumeURLToken()\n            }\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["return"]]," FunctionToken, l.r.Shift()\n    }\n\n    ",["$","span",{"class":"hljs-keyword"},["return"]]," IdentToken, l.r.Shift()\n}"]]]],["$","p",{},["注意这里的实现其实会在含转义的 URL-token 上出现问题，后续通过修改 consumeName 函数的实现，通过返回值判断解决了此问题。"]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume_token.go#L70-L112","rel":"external nofollow noreferrer"},[["$","code",{},["consumeStringToken()"]]]]]],["$","ul",{},[["$","li",{},["简而言之，就是从开始的引号的位置一直匹配到相对应的结束引号位置或者文件末尾；"]],["$","li",{},["特别地，如果遇到没有转义的换行，那么此时就需要作为 bad-string-token 返回了。"]]]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-string-token"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeStringToken() (TokenType, []",["$","span",{"class":"hljs-type"},["rune"]],") {\n    until := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],") ",["$","span",{"class":"hljs-comment"},["// the opening quote, already checked valid by the caller"]],"\n    l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," {\n        next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == until {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n            ",["$","span",{"class":"hljs-keyword"},["return"]]," StringToken, l.r.Shift()\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == EOF {\n            ",["$","span",{"class":"hljs-keyword"},["return"]]," StringToken, l.r.Shift()\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," isCSSNewline(next) {\n            ",["$","span",{"class":"hljs-keyword"},["return"]]," BadStringToken, l.r.Shift()\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'\\\\'"]]," {\n            next_next := l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")\n\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," next_next == EOF {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the backslash"]],"\n                ",["$","span",{"class":"hljs-keyword"},["continue"]],"\n            }\n\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," isCSSNewline(next_next) {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n                l.consumeSingleWhitespace()\n            } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," twoCharsAreValidEscape(next, next_next) {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the backslash"]],"\n                l.consumeEscape()\n            } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n            }\n        } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the current rune"]],"\n        }\n    }\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume_token.go#L114-L164","rel":"external nofollow noreferrer"},[["$","code",{},["consumeURLToken()"]]]]]],["$","ul",{},[["$","li",{},["需要按照规范特别注意 bad-url-token 的情况。"]],["$","li",{},["但此处的实现和规范不同，在 ",["$","code",{},["consumeIdentLikeToken()"]]," 中我们把 URL 的前导空格全部 consume 掉了，但如果遇到使用引号包裹的 URL 时，这段空格理应单独作为一个 whitespace-token，不过无伤大雅，这样解析也可以，不影响后续的 parse 流程。"]]]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-url-token"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeURLToken() (TokenType, []",["$","span",{"class":"hljs-type"},["rune"]],") {\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," {\n        next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["')'"]]," {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n            ",["$","span",{"class":"hljs-keyword"},["return"]]," UrlToken, l.r.Shift()\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == EOF {\n            ",["$","span",{"class":"hljs-keyword"},["return"]]," UrlToken, l.r.Shift()\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," isHTMLWhitespace(next) {\n            l.consumeWhitespace()\n\n            next_next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," next_next == ",["$","span",{"class":"hljs-string"},["')'"]]," {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the closing parenthesis"]],"\n                ",["$","span",{"class":"hljs-keyword"},["return"]]," UrlToken, l.r.Shift()\n            }\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," next_next == EOF {\n                ",["$","span",{"class":"hljs-keyword"},["return"]]," UrlToken, l.r.Shift()\n            }\n\n            ",["$","span",{"class":"hljs-comment"},["// If the next character is not a closing parenthesis, there's an error and we should mark it as a bad URL token."]],"\n            ",["$","span",{"class":"hljs-keyword"},["break"]],"\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'\"'"]]," || next == ",["$","span",{"class":"hljs-string"},["'\\''"]]," || isNonPrintableCodePoint(next) {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the invalid character"]],"\n            ",["$","span",{"class":"hljs-keyword"},["break"]],"\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'\\\\'"]]," {\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," twoCharsAreValidEscape(next, l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")) {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the backslash"]],"\n                l.consumeEscape()\n                ",["$","span",{"class":"hljs-keyword"},["continue"]],"\n            } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n                ",["$","span",{"class":"hljs-keyword"},["break"]],"\n            }\n        }\n\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the current rune"]],"\n    }\n\n    l.consumeBadUrlRemnants()\n    ",["$","span",{"class":"hljs-keyword"},["return"]]," BadUrlToken, l.r.Shift()\n}"]]]]]]]],["$","h3",{"id":"特定类型字符片段解析"},["特定类型字符片段解析"]],["$","p",{},["一共有以下几个片段解析的函数："]],["$","ul",{},[["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L3-L19","rel":"external nofollow noreferrer"},[["$","code",{},["consumeUntilCommentEnd()"]]]],"：一直读取到注释结束。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-comment"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeUntilCommentEnd() {\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," {\n        next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == EOF {\n            ",["$","span",{"class":"hljs-keyword"},["break"]],"\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'*'"]]," && l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],") == ",["$","span",{"class":"hljs-string"},["'/'"]]," {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["2"]],") ",["$","span",{"class":"hljs-comment"},["// consume '*/'"]],"\n            ",["$","span",{"class":"hljs-keyword"},["return"]],"\n        }\n\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the current rune"]],"\n    }\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L21-L42","rel":"external nofollow noreferrer"},[["$","code",{},["consumeEscape()"]]]],"：解析一个转义字符。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-escaped-code-point"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeEscape() ",["$","span",{"class":"hljs-type"},["rune"]]," {\n    ",["$","span",{"class":"hljs-keyword"},["var"]]," res ",["$","span",{"class":"hljs-type"},["rune"]]," = ",["$","span",{"class":"hljs-number"},["0"]],"\n\n    next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," isASCIIHexDigit(next) {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n        res = hexDigitToValue(next)\n\n        ",["$","span",{"class":"hljs-keyword"},["for"]]," i := ",["$","span",{"class":"hljs-number"},["1"]],"; i < ",["$","span",{"class":"hljs-number"},["6"]],"; i++ {\n            c := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n            ",["$","span",{"class":"hljs-keyword"},["if"]]," isASCIIHexDigit(c) {\n                l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n                res = res*",["$","span",{"class":"hljs-number"},["16"]]," + hexDigitToValue(c)\n            } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n                ",["$","span",{"class":"hljs-keyword"},["break"]],"\n            }\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," !isValidCodePoint(res) {\n            res = ",["$","span",{"class":"hljs-string"},["'\\uFFFD'"]]," ",["$","span",{"class":"hljs-comment"},["// U+FFFD REPLACEMENT CHARACTER"]],"\n        }\n\n        ",["$","span",{"class":"hljs-comment"},["// If the next input code point is whitespace, consume it as well."]],"\n        l.consumeSingleWhitespace()\n    } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," next != EOF {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the escape character"]],"\n        res = next\n    } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n        res = ",["$","span",{"class":"hljs-string"},["'\\uFFFD'"]]," ",["$","span",{"class":"hljs-comment"},["// U+FFFD REPLACEMENT CHARACTER for EOF"]],"\n    }\n\n    ",["$","span",{"class":"hljs-keyword"},["return"]]," res\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L44-L58","rel":"external nofollow noreferrer"},[["$","code",{},["consumeName()"]]]],"：读取一个 name。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-name"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeName() {\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," {\n        next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," isNameCodePoint(next) {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n        } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," twoCharsAreValidEscape(next, l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")) {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the backslash"]],"\n            l.consumeEscape()\n        } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n            ",["$","span",{"class":"hljs-keyword"},["break"]],"\n        }\n    }\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L60-L92","rel":"external nofollow noreferrer"},[["$","code",{},["consumeNumber()"]]]],"：读取一个数字。需要特别注意对科学计数法的处理，以及与调用侧配合正确解析 ",["$","code",{},[".7"]]," ",["$","code",{},["+.7"]]," 等 case。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-number"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeNumber() {\n    next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n    ",["$","span",{"class":"hljs-comment"},["// If the next rune is '+' or '-', consume it as part of the number."]],"\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'+'"]]," || next == ",["$","span",{"class":"hljs-string"},["'-'"]]," {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n    }\n\n    ",["$","span",{"class":"hljs-comment"},["// consume the integer part of the number"]],"\n    l.r.MoveWhilePredicate(isASCIIDigit)\n\n    ",["$","span",{"class":"hljs-comment"},["// float"]],"\n    next = l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'.'"]]," && isASCIIDigit(l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")) {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the '.'"]],"\n        l.r.MoveWhilePredicate(isASCIIDigit)\n    }\n\n    ",["$","span",{"class":"hljs-comment"},["// scientific notation"]],"\n    next = l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'e'"]]," || next == ",["$","span",{"class":"hljs-string"},["'E'"]]," {\n        next_next := l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," isASCIIDigit(next_next) {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume 'e' or 'E'"]],"\n            l.r.MoveWhilePredicate(isASCIIDigit)\n        } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," (next_next == ",["$","span",{"class":"hljs-string"},["'+'"]]," || next_next == ",["$","span",{"class":"hljs-string"},["'-'"]],") && isASCIIDigit(l.r.Peek(",["$","span",{"class":"hljs-number"},["2"]],")) {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["2"]],") ",["$","span",{"class":"hljs-comment"},["// consume 'e' or 'E' and the sign"]],"\n            l.r.MoveWhilePredicate(isASCIIDigit)\n        }\n    }\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L94-L101","rel":"external nofollow noreferrer"},[["$","code",{},["consumeSingleWhitespace()"]]]],"：读取一个空格。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeSingleWhitespace() {\n    next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n    ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["'\\r'"]]," && l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],") == ",["$","span",{"class":"hljs-string"},["'\\n'"]]," {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["2"]],") ",["$","span",{"class":"hljs-comment"},["// consume CRLF"]],"\n    } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," isHTMLWhitespace(next) {\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the whitespace character"]],"\n    }\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L103-L115","rel":"external nofollow noreferrer"},[["$","code",{},["consumeWhitespace()"]]]],"：读取多个空格。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeWhitespace() {\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," {\n        next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," isHTMLWhitespace(next) {\n            l.consumeSingleWhitespace()\n        } ",["$","span",{"class":"hljs-keyword"},["else"]]," ",["$","span",{"class":"hljs-keyword"},["if"]]," next == EOF {\n            ",["$","span",{"class":"hljs-keyword"},["return"]],"\n        } ",["$","span",{"class":"hljs-keyword"},["else"]]," {\n            ",["$","span",{"class":"hljs-keyword"},["break"]],"\n        }\n    }\n}"]]]]]],["$","li",{},[["$","p",{},[["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/7c4a62d23d98865692e3633de64db503b56f6556/consume.go#L117-L138","rel":"external nofollow noreferrer"},[["$","code",{},["consumeBadUrlRemnants()"]]]],"：读取 bad-url-token 的剩余部分。"]],["$","pre",{},[["$","code",{"class":"hljs go"},[["$","span",{"class":"hljs-comment"},["// https://www.w3.org/TR/2021/CRD-css-syntax-3-20211224/#consume-the-remnants-of-a-bad-url"]],"\n",["$","span",{"class":"hljs-function"},[["$","span",{"class":"hljs-keyword"},["func"]]," ",["$","span",{"class":"hljs-params"},["(l *Lexer)"]]]]," consumeBadUrlRemnants() {\n    ",["$","span",{"class":"hljs-keyword"},["for"]]," {\n        next := l.r.Peek(",["$","span",{"class":"hljs-number"},["0"]],")\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == ",["$","span",{"class":"hljs-string"},["')'"]]," {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n            ",["$","span",{"class":"hljs-keyword"},["return"]],"\n        }\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," next == EOF {\n            ",["$","span",{"class":"hljs-keyword"},["return"]],"\n        }\n\n        ",["$","span",{"class":"hljs-keyword"},["if"]]," twoCharsAreValidEscape(next, l.r.Peek(",["$","span",{"class":"hljs-number"},["1"]],")) {\n            l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],") ",["$","span",{"class":"hljs-comment"},["// consume the backslash"]],"\n            l.consumeEscape()\n            ",["$","span",{"class":"hljs-keyword"},["continue"]],"\n        }\n\n        l.r.Move(",["$","span",{"class":"hljs-number"},["1"]],")\n    }\n}"]]]]]]]],["$","h3",{"id":"小结"},["小结"]],["$","p",{},["让我们来总结一下 lexer 工作流程：在 lexer 读取到某个 token 的起始点的时候，lexer 预读起始的几个字符，然后辨别 token 的类型。对于大致分类好的 token，根据其更具体的特征预读并消耗掉对应的字符，直到这个 token 结束。"]],["$","p",{},["大致的类型辨别是通过 ",["$","code",{},["Next()"]]," 函数中的那个巨大的 switch-case 语句来完成的。而对于精细的 token 类型的判断，则是 case 中的语句和 ",["$","code",{},["consume_token.go"]]," 定义的一系列函数来共同完成的。至于 token 内部的字符段的解析，则是 ",["$","code",{},["consume.go"]]," 中的一系列函数完成的。由此组合，整个 token 的解析过程得以良好运转。"]],["$","h2",{"id":"测试"},["测试"]],["$","p",{},["为了验证 lexer 的实现正确性，我们引入了 ",["$","a",{"href":"https://github.com/romainmenke/css-tokenizer-tests/tree/5e2112b59e728205a870ff130987e5204c425f59/tests","rel":"external nofollow noreferrer"},["romainmenke/css-tokenizer-tests"]]," 的测试用例来对 lexer 进行测试。具体的测试流程可以参考 ",["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer/blob/master/lexer_test.go","rel":"external nofollow noreferrer"},[["$","code",{},["lexer_test.go"]]]]," 中的实现。"]],["$","p",{},["根据测试结果来看，出现的问题主要集中在与转义字符相关的处理，对于大部分情况已经能够正常解析。截止编写本文之时，测试通过率为 96.53% (167/173)，个人认为已经处于可用水平。"]],["$","h2",{"id":"后记"},["后记"]],["$","p",{},["文中所述的 lexer 的具体实现已经开源在 ",["$","a",{"href":"https://github.com/renbaoshuo/go-css-lexer","rel":"external nofollow noreferrer"},["renbaoshuo/go-css-lexer"]],"，欢迎大家 Star！"]],["$","p",{},["搓这个 lexer 花了半个周末的时间，修修补补又消耗了一些时间。也算是在工作之余充实自己的大脑了。"]],["$","p",{},["文章题图由 Gemini 2.5 Pro Imagen 生成。"]]],"thumb":"https://s1.baoshuo.ren/2025/08/05/GeYRiDgPyqwrCLU.jpg","date":"2025-08-05","updated":"2025-08-05","isoDate":"2025-08-05T15:18:52.000Z","isoUpdate":"2025-08-05T15:29:04.000Z","categories":[{"name":"技术向","url":"/categories/%E6%8A%80%E6%9C%AF%E5%90%91/"}],"tags":[{"name":"CSS","url":"/tags/CSS/"},{"name":"编译原理","url":"/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"}],"license":null,"permalink":"https://blog.baoshuo.ren/post/css-lexer/","url":"/post/css-lexer/","prev":{"title":"向着璀璨的未来进发 —— 我的 2024 年度总结","url":"/post/goodbye-2024/"},"next":{"title":"愿此去前路皆坦途 —— 我的 2023 年度总结","url":"/post/goodbye-2023/"},"toc":{"0":{"text":"词法分析","id":"词法分析"},"1":{"text":"Token 的分类","id":"token-的分类"},"2":{"text":"输入流","id":"输入流"},"3":{"0":{"text":"Token 开始处的分类讨论","id":"token-开始处的分类讨论"},"1":{"text":"Token 解析","id":"token-解析"},"2":{"text":"特定类型字符片段解析","id":"特定类型字符片段解析"},"3":{"text":"小结","id":"小结"},"text":"词法分析器","id":"词法分析器"},"4":{"text":"测试","id":"测试"},"5":{"text":"后记","id":"后记"}},"hasToc":true,"comments":true,"wordCount":"约 4.6 千字"}},"__N_SSG":true}